{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paragraph level alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from util import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(fname, outname):\n",
    "    with open(fname, 'r') as f:\n",
    "        data = f.read()\n",
    "    with open(outname, 'w') as f:\n",
    "        for line in data.split('\\n'):\n",
    "            if line == \"<p>\": f.write(line+'\\n')\n",
    "            else:\n",
    "                f.write(tokenize_new(line)+'\\n')\n",
    "tokenize_file(\"night2.bo.txt\", \"night2_paragraph.bo.stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_file_en(fname, outname):\n",
    "    with open(fname, 'r') as f:\n",
    "        data = f.read()\n",
    "    with open(outname, 'w') as f:\n",
    "        for line in data.split('\\n'):\n",
    "            if line == \"<p>\": f.write(line+'\\n')\n",
    "            else:\n",
    "                f.write(en_tokenize(line)+'\\n')\n",
    "tokenize_file_en(\"night2.en.txt\", \"night2_paragraph.en.stem\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "../hunalign/src/hunalign/hunalign null.dic night2_paragraph.en.stem night2_paragraph.bo.stem -realign -autodict=night_paragraph.autodict > night_align.num\n",
    "Global quality of unfiltered align after realign 0.0.791287\n",
    "Quality 0.0.791287\n",
    "\n",
    "\n",
    "../hunalign/src/hunalign/hunalign ../illum.dic night2_paragraph.en.stem night2_paragraph.bo.stem -realign -autodict=night_paragraph2.autodict > night_align2.num\n",
    "Global quality of unfiltered align after realign 0.769245\n",
    "Quality 0.769245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build text pairs\n",
    "rebuild_text('night2.en.txt', 'night2.bo.txt', \"night_align.num\", \"night-para-align.txt\", 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rebuild_text_df('night2.en.txt', 'night2.bo.txt', \"night_align.num\", 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['source', 'target', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source_url'] = 'NA'\n",
    "df['target_url'] = 'https://latse.org/108-translations/#Night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('bisent_night_paragraph_en-bo.csv', index=False)\n",
    "df.sample(30, random_state=32).to_csv('bisent_night_paragraph_en-bo_sample30.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intra-paragraph (sentence) alignment (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_intraparagraph_basic(df, \"camille-intraparagraph.fr.txt\", \"camille-intraparagraph.bo.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_intraparagraph(df, \"camille-intraparagraph.fr.txt\", \"camille-intraparagraph.bo.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target = \"camille-intraparagraph.fr.txt\", \"camille-intraparagraph.bo.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 ['ངས་ནན་བརྗོད་བྱ་དགོས་པ་ཞིག་ལ་སྒྲུང་འདིར་བཀོད་པའི་དོན་དངོས་ཕལ་ཆེ་བར་པ་རི་སི་གྲོང་ཁྱེར་དུ་བདེན་དཔང་བྱེད་ཐུབ་མཁན་ཡོད་ཅིང་། གལ་སྲིད་ངས་བདེན་པར་བརྗོད་པ་འདིར་ཆ་འཇོག་དཀའ་ན་', 'སྐྱེ་བོ་དེ་དག་ལ་དྲིས་ན་', 'ངེས་གཏན་བྱ་ཐུབ།', 'ང་ནི་རྒྱུ་རྐྱེན་ཁྱད་པར་བ་ཞིག་གིས་སྒྲུང་འདིའི་མགོ་མཇུག་བར་གསུམ་གྱི་བྱུང་རིམ་ཆ་ཚང་ཞིག་བཤད་ཐུབ་པའི་སྐྱེ་བོ་འབའ་ཞིག་དེ་ཡིན།', 'ཅིའི་ཕྱིར་ཞེ་ན་ང་སྒྲུང་འདིའི་མཐའ་མཇུག་གི་བྱུང་རིམ་གྱི་ཞིབ་ཕྲ་ཁག་ལ་ཆ་རྒྱུས་ཡོད་པའི་མི་གཅིག་པུ་དེ་ཡང་ཡིན།', 'ངས་ཤེས་པའི་བྱུང་རིམ་གྱི་ཞིབ་ཆ་དེ་དག་མེད་ན་སྒྲུང་འདི་སྙན་པོ་དང་ཆ་ཚང་ཞིག་མི་འགྲུབ།'] [\"D'ailleurs, il y a à Paris des témoins de la plupart des faits que je recueille ici, et qui pourraient les confirmer, si mon témoignage ne suffisait pas.\", \"Par une circonstance particulière, seul je pouvais les écrire, car seul j'ai été le confident des derniers détails sans lesquels il eût été impossible de faire un récit intéressant et complet.\"]\n"
     ]
    }
   ],
   "source": [
    "source_tok = source.replace(\".txt\", \".stem\")\n",
    "target_tok = target.replace(\".txt\", \".stem\")\n",
    "with open(source,'w') as f_s, open(source_tok, 'w') as f_st,open(target,'w') as f_t, open(target_tok, 'w') as f_tt:\n",
    "    for i, row in df.iterrows():\n",
    "        en_sents = en_get_sents(row['source'])\n",
    "        bo_sents_tok, bo_sents = bo_get_sents(row[\"target\"])\n",
    "        if len(bo_sents_tok) == 1 or len(en_sents) == 1:\n",
    "            continue\n",
    "        f_s.write(\"\\n\".join(en_sents) + \"\\n<p>\\n\")\n",
    "        f_st.write(\"\\n\".join(en_tokenize(s) for s in en_sents) + \"\\n<p>\\n\")\n",
    "        f_t.write(\"\\n\".join(bo_sents) + \"\\n<p>\\n\")\n",
    "        f_tt.write(\"\\n\".join(bo_sents_tok) + \"\\n<p>\\n\")\n",
    "        print(i, bo_sents, en_sents)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something is wrong with tokenizer here:\n",
    "s = '''ངས་ནན་བརྗོད་བྱ་དགོས་པ་ཞིག་ལ་སྒྲུང་འདིར་བཀོད་པའི་དོན་དངོས་ཕལ་ཆེ་བར་པ་རི་སི་གྲོང་ཁྱེར་དུ་བདེན་དཔང་བྱེད་ཐུབ་མཁན་ཡོད་ཅིང་། གལ་སྲིད་ངས་བདེན་པར་བརྗོད་པ་འདིར་ཆ་འཇོག་དཀའ་ན་སྐྱེ་བོ་དེ་དག་ལ་དྲིས་ན་ངེས་གཏན་བྱ་ཐུབ།'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ང་ ནན་ བརྗོད་བྱ་ དགོས་པ་ ཞིག་ ལ་ སྒྲུང་ འདི་ ལ་ བཀོད་པ་ གི་ དོན་ དངོ་ ཕལ་ཆེ་བ་ ར་པ་ རི་ སི་ གྲོང་ཁྱེར་ དུ་ བདེན་དཔང་ བྱེད་ ཐུབ་ མཁན་ ཡོད་ ཅིང་ །  གལ་སྲིད་ ང་ བདེན་པ་ ལ་ བརྗོད་པ་ འདི་ ལ་ ཆ་འཇོག་ དཀའ་ ན་',\n",
       "  'སྐྱེ་བོ་ དེ་དག་ ལ་ དྲི་ ན་',\n",
       "  'ངེས་ གཏན་ བྱ་ ཐུབ་ །'],\n",
       " ['ངས་ནན་བརྗོད་བྱ་དགོས་པ་ཞིག་ལ་སྒྲུང་འདིར་བཀོད་པའི་དོན་དངོས་ཕལ་ཆེ་བར་པ་རི་སི་གྲོང་ཁྱེར་དུ་བདེན་དཔང་བྱེད་ཐུབ་མཁན་ཡོད་ཅིང་། གལ་སྲིད་ངས་བདེན་པར་བརྗོད་པ་འདིར་ཆ་འཇོག་དཀའ་ན་',\n",
       "  'སྐྱེ་བོ་དེ་དག་ལ་དྲིས་ན་',\n",
       "  'ངེས་གཏན་བྱ་ཐུབ།'])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bo_get_sents(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "ང་ ནན་ བརྗོད་བྱ་ དགོས་པ་ ཞིག་ ལ་ སྒྲུང་ འདི་ ལ་ བཀོད་པ་ གི་ དོན་ དངོ་ ཕལ་ཆེ་བ་ ར་པ་ རི་ སི་ གྲོང་ཁྱེར་ དུ་ བདེན་དཔང་ བྱེད་ ཐུབ་ མཁན་ ཡོད་ ཅིང་ །  གལ་སྲིད་ ང་ བདེན་པ་ ལ་ བརྗོད་པ་ འདི་ ལ་ ཆ་འཇོག་ དཀའ་ ན་\n",
      "ངས་ནན་བརྗོད་བྱ་དགོས་པ་ཞིག་ལ་སྒྲུང་འདིར་བཀོད་པའི་དོན་དངོས་ཕལ་ཆེ་བར་པ་རི་སི་གྲོང་ཁྱེར་དུ་བདེན་དཔང་བྱེད་ཐུབ་མཁན་ཡོད་ཅིང་། གལ་སྲིད་ངས་བདེན་པར་བརྗོད་པ་འདིར་ཆ་འཇོག་དཀའ་ན་\n",
      "--\n",
      "སྐྱེ་བོ་ དེ་དག་ ལ་ དྲི་ ན་\n",
      "སྐྱེ་བོ་དེ་དག་ལ་དྲིས་ན་\n",
      "--\n",
      "ངེས་ གཏན་ བྱ་ ཐུབ་ །\n",
      "ངེས་གཏན་བྱ་ཐུབ།\n"
     ]
    }
   ],
   "source": [
    "sents = sentence_tok(s)\n",
    "for i in range(len(sentence_tok(s))):\n",
    "    toks = sents[i][1]\n",
    "    if toks:\n",
    "        print(\"--\")\n",
    "        print( \" \".join( t.lemma if t.lemma else t.text for t in toks).strip() )\n",
    "        print( s[toks[0].start:toks[-1].start+toks[-1].len].strip() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ངེས་ གཏན་ བྱ་ ཐུབ་ །'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join( t.lemma if t.lemma else t.text for t in toks).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ངས་ནན་བརྗོད་བྱ་དགོས་པ་ཞིག་ལ་སྒྲུང་འདིར་བཀོད་པའི་དོན་དངོས་ཕལ་ཆེ་བར་པ་རི་སི་གྲོང་ཁྱེར་དུ་བདེན་དཔང་བྱེད་ཐུབ་མཁན་ཡོད་ཅིང་། གལ་སྲིད་ངས་བདེན་པར་བརྗོད་པ་འདིར་ཆ་འཇོག་དཀའ་ན་སྐྱེ་བོ་དེ་དག་ལ་དྲིས་ན་ངེས་གཏན་བྱ་ཐུབ།\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
